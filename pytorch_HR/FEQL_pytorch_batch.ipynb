{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEQL for hard rod (hard sphere in 1D) pytorch prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    from IPython.core.display import display, HTML\n",
    "    display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose GPU pr cpu\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    " \n",
    "# The GPU id to use, usually either \"0\" or \"1\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\";  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import grad\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "#torch.cuda.current_device()\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "device = 'cpu'\n",
    "if(torch.cuda.is_available()):\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "    device='cuda'\n",
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports \n",
    "from scipy.integrate import simps\n",
    "from numpy import exp, absolute\n",
    "from numpy import exp, asarray, empty\n",
    "from numpy import zeros, array, float, random\n",
    "import itertools as it\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from shutil import copyfile\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy\n",
    "from sympy import Matrix\n",
    "from sympy import symbols\n",
    "from sympy import diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility import load_data,train_val_data,train_varible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = '../HR_data/'\n",
    "L,N,dx,batch_size,MC_inform = load_data(path_to_file)\n",
    "input_shape = (N,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho_train, rho_test ,c1_HR_train, c1_HR_test,Vext_train, \\\n",
    "Vext_test,eps_train,eps_test, mu_train, mu_test, deltaF_train, deltaF_test = train_val_data(path_to_file,N,MC_inform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert data to train_val tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.from_numpy(rho_train),torch.from_numpy(Vext_train),torch.from_numpy(mu_train))\n",
    "train_loader=DataLoader(train_dataset,batch_size=64, shuffle=True)\n",
    "val_dataset = TensorDataset(torch.from_numpy(rho_test),torch.from_numpy(Vext_test),torch.from_numpy(mu_test))\n",
    "val_loader=DataLoader(val_dataset,batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(np.linspace(0,N-1,N)*dx,rho_train[0],label=r\"$\\rho_1$\")\n",
    "#plt.plot(np.linspace(0,N-1,N)*dx,rho_test[0],label=r\"$\\rho_2$\")\n",
    "#plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layer=2\n",
    "n_density=2\n",
    "n_parameter = 0\n",
    "n_id =1\n",
    "n_log = 5\n",
    "n_exp = 0\n",
    "n_mul = 1\n",
    "n_div = 0\n",
    "n_conv = n_density*(1+n_parameter)\n",
    "n_dim1 = n_id+n_log+n_exp+n_mul*2+n_div*2\n",
    "n_dim1p= n_id+n_log+n_exp+n_mul+n_div"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sympy to find eqaution and redundant part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonlinear(z,os): \n",
    "    y=[]\n",
    "    if(os=='torch'):\n",
    "        exp = torch.exp\n",
    "        log = torch.log\n",
    "        cz = lambda x:z[:,x]\n",
    "    if(os=='sympy'):\n",
    "        exp = sympy.exp\n",
    "        log = sympy.log\n",
    "        cz = lambda x:z[x]\n",
    "    \n",
    "    for i in range (n_id):\n",
    "        y.append(cz(i))\n",
    "    for i in range (n_id,n_id+n_exp):\n",
    "        y.append(exp(cz(i))-1)\n",
    "    for i in range (n_id+n_exp,n_id+n_exp+n_log):\n",
    "        y.append(log(cz(i)+1))\n",
    "    for i in range (n_id+n_exp+n_log,n_id+n_exp+n_log+n_mul*2,2):\n",
    "        y.append(cz(i)*cz(i+1))\n",
    "    for i in range (n_id+n_exp+n_log+n_mul*2,n_id+n_exp+n_log+n_mul*2+n_div*2,2):\n",
    "        y.append(cz(i)/(cz(i+1)+1))\n",
    "   \n",
    "    if(os=='torch'):\n",
    "        return torch.stack(y,dim=1)\n",
    "    if(os=='sympy'):\n",
    "        return Matrix(len(y),1,y)\n",
    "\n",
    "    print(\"Not supported\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_nonlinear_sympy():\n",
    "    for layer in range (1,n_layer+1):\n",
    "        if(layer==1):\n",
    "            sn=Matrix(n_conv,1,symbols('n0:'+str(n_conv)))\n",
    "            W=Matrix(n_dim1,n_conv,symbols('a'+str(layer)+'L0:'+str(n_dim1*n_conv)))\n",
    "            y=W*sn\n",
    "            z=nonlinear(y,os='sympy')\n",
    "        else:\n",
    "            W=Matrix(n_dim1,n_dim1p,symbols('a'+str(layer)+'L0:'+str(n_dim1*n_dim1p)))\n",
    "            y=W*z\n",
    "            z=nonlinear(y,os='sympy')\n",
    "    return z,sn\n",
    "res,sn = linear_nonlinear_sympy()\n",
    "res=sympy.ones(1,res.shape[0])*res\n",
    "fed = res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle a2L0 \\left(a1L0 n_{0} + a1L1 n_{1}\\right) + a2L1 \\log{\\left(a1L2 n_{0} + a1L3 n_{1} + 1 \\right)} + a2L2 \\left(a1L4 n_{0} + a1L5 n_{1}\\right) \\left(a1L6 n_{0} + a1L7 n_{1}\\right) + \\left(a2L10 \\log{\\left(a1L2 n_{0} + a1L3 n_{1} + 1 \\right)} + a2L11 \\left(a1L4 n_{0} + a1L5 n_{1}\\right) \\left(a1L6 n_{0} + a1L7 n_{1}\\right) + a2L9 \\left(a1L0 n_{0} + a1L1 n_{1}\\right)\\right) \\left(a2L6 \\left(a1L0 n_{0} + a1L1 n_{1}\\right) + a2L7 \\log{\\left(a1L2 n_{0} + a1L3 n_{1} + 1 \\right)} + a2L8 \\left(a1L4 n_{0} + a1L5 n_{1}\\right) \\left(a1L6 n_{0} + a1L7 n_{1}\\right)\\right) + \\log{\\left(a2L3 \\left(a1L0 n_{0} + a1L1 n_{1}\\right) + a2L4 \\log{\\left(a1L2 n_{0} + a1L3 n_{1} + 1 \\right)} + a2L5 \\left(a1L4 n_{0} + a1L5 n_{1}\\right) \\left(a1L6 n_{0} + a1L7 n_{1}\\right) + 1 \\right)}$"
      ],
      "text/plain": [
       "a2L0*(a1L0*n0 + a1L1*n1) + a2L1*log(a1L2*n0 + a1L3*n1 + 1) + a2L2*(a1L4*n0 + a1L5*n1)*(a1L6*n0 + a1L7*n1) + (a2L10*log(a1L2*n0 + a1L3*n1 + 1) + a2L11*(a1L4*n0 + a1L5*n1)*(a1L6*n0 + a1L7*n1) + a2L9*(a1L0*n0 + a1L1*n1))*(a2L6*(a1L0*n0 + a1L1*n1) + a2L7*log(a1L2*n0 + a1L3*n1 + 1) + a2L8*(a1L4*n0 + a1L5*n1)*(a1L6*n0 + a1L7*n1)) + log(a2L3*(a1L0*n0 + a1L1*n1) + a2L4*log(a1L2*n0 + a1L3*n1 + 1) + a2L5*(a1L4*n0 + a1L5*n1)*(a1L6*n0 + a1L7*n1) + 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp=0\n",
    "for n in sn:\n",
    "    subs=diff(fed,n)\n",
    "    for m in sn:\n",
    "        subs=subs.subs({m:0})\n",
    "    tmp+=subs\n",
    "redunds=str(tmp.free_symbols)[1:-1].split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_mask = torch.ones(n_dim1,n_dim1p)\n",
    "for redund in redunds:\n",
    "    idx = [redund.find('a'),redund.find('L')]\n",
    "    layer = int(redund[idx[0]+1:idx[1]])\n",
    "    order = int(redund[idx[1]+1:])\n",
    "    #print(layer,order)\n",
    "    if(layer==n_layer):\n",
    "        #print(layer,order)\n",
    "        row = order//n_dim1p\n",
    "        column = order%n_dim1p\n",
    "        mu_mask[row][column]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_mask=mu_mask.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial\n",
    "## Initial weight dims and FMT weight functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7efbd353d070>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_dim = N//2+1\n",
    "torch.manual_seed(420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w_FMT():\n",
    "    R = 1.0/2\n",
    "    k=np.linspace(0,N//2,N//2+1)*2*np.pi/L\n",
    "    w0=2*np.cos(k*R)/2\n",
    "    k[0]=1 #keep notebook shutup\n",
    "    w1=2*np.sin(k*R)/k\n",
    "    w1[0]=2*R\n",
    "    return w0,w1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution and cross correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_multiplication(t1, t2,cross):\n",
    "    real1, imag1 = t1.t()\n",
    "    real2, imag2 = t2\n",
    "    if(cross):\n",
    "        imag2=imag2*-1\n",
    "    #print(imag1.device,imag2.device)\n",
    "    return torch.stack([real1 * real2 - imag1 * imag2, real1 * imag2 + imag1 * real2], dim = -1)\n",
    "\n",
    "def conv(rho,w,cross=False):\n",
    "    frho = torch.rfft(rho, 1,  onesided=True)\n",
    "    c=complex_multiplication(frho, w,cross)\n",
    "    c=torch.irfft(c, 1,signal_sizes=rho.shape)\n",
    "    return c\n",
    "\n",
    "\n",
    "def simple_conv(rhos,w,cross=False):\n",
    "    frho = torch.rfft(rhos, 1,  onesided=True)\n",
    "    #print(frho.shape)\n",
    "    real1,imag1 = frho[:,:,0],frho[:,:,1]\n",
    "    real2,imag2=w\n",
    "    if(cross):\n",
    "        c = torch.stack([real1 * real2 - imag1 * (-imag2), real1 * (-imag2) + imag1 * real2], dim = -1)\n",
    "    else:\n",
    "        c = torch.stack([real1 * real2 - imag1 * imag2, real1 * imag2 + imag1 * real2], dim = -1)\n",
    "    c = torch.irfft(c, 1,signal_sizes=rhos[0].shape)\n",
    "    #print(c.shape)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## determine $\\mu^{ML}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_mu(rhos,rhoMLs):\n",
    "    return torch.log(torch.sum(rhos,dim=-1)/torch.sum(rhoMLs,dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mm(dense,ys): #matrix mul list\n",
    "    z=[]\n",
    "    for i in range(len(dense)):\n",
    "        tmp=dense[i][0]*ys[0]\n",
    "        for j in range(1,len(dense[0])):\n",
    "            tmp+=dense[i][j]*ys[j]\n",
    "        z.append(tmp)\n",
    "    return torch.stack(z,dim=1)\n",
    "\n",
    "def linear_nonlinear_list(dense,ys):\n",
    "    z=mm(dense,ys)\n",
    "    #print(dense.shape,z.shape)\n",
    "    ys=nonlinear(z,os='torch')\n",
    "    return ys\n",
    "\n",
    "def linear_nonlinear(dense,ys):\n",
    "    #print(dense.shape,ys.shape)\n",
    "    z=torch.matmul(dense,ys)\n",
    "    #print(z.shape)\n",
    "    ys=nonlinear(z,os='torch')\n",
    "    return ys\n",
    "\n",
    "\n",
    "def mask_fix(dense,layer):\n",
    "    mask=torch.zeros_like(dense,device=device)\n",
    "    idx = torch.where(torch.abs(dense)>kill)\n",
    "    mask[idx]=1.\n",
    "    return mask\n",
    "\n",
    "\n",
    "def mask_gen(dense,kill):\n",
    "    mask=torch.zeros_like(dense,device=device)\n",
    "    idx = torch.where(torch.abs(dense)>kill)\n",
    "    mask[idx]=1.\n",
    "    return mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_equation():\n",
    "    layer = 0\n",
    "    sn=Matrix(n_conv,1,symbols('n0:'+str(n_conv)))\n",
    "    w_tmp = mask_gen(dense_layer_0,kill)*dense_layer_0\n",
    "    #print(mask_gen(dense_layer_0[0],kill),dense_layer_0,w_tmp)\n",
    "    W=Matrix(w_tmp.cpu().detach().numpy())\n",
    "    y=W*sn\n",
    "    z=nonlinear(y,os='sympy')\n",
    "    #print(np.matrix(z))\n",
    "    for layer in range (0,n_layer-1):\n",
    "        w_tmp = dense_layer[layer]*mask_gen(dense_layer[layer],kill)\n",
    "        if(layer==n_layer-2):\n",
    "            w_tmp*=mu_mask\n",
    "            #print(mu_mask)\n",
    "        #print(mask_gen(dense_layer[layer],kill),dense_layer[layer],w_tmp)\n",
    "        W=Matrix(w_tmp.cpu().detach().numpy())\n",
    "        #print(np.matrix(W))\n",
    "        y=W*z\n",
    "        z=nonlinear(y,os='sympy')\n",
    "        #print(np.matrix(z))\n",
    "    res =z\n",
    "    #print(res.shape)\n",
    "    fed=sympy.ones(1,res.shape[0])*res\n",
    "    return fed[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FEQL(rhos,Vexts,kill):\n",
    "    #ns=[]\n",
    "    ns=[]\n",
    "    for i in range(n_conv):\n",
    "        tmp = simple_conv(rhos,w_torch[i])\n",
    "        ns.append(tmp) \n",
    "    ####\n",
    "    #### don't stack ns, I don't why either !!!\n",
    "    ####\n",
    "    \n",
    "    mask0=mask_gen(dense_layer_0,kill)\n",
    "    mask=mask_gen(dense_layer,kill)\n",
    "    #print(dense_layer_0,dense_layer_0*mask0)\n",
    "    \n",
    "    ys=linear_nonlinear_list(dense_layer_0*mask0,ns)\n",
    "    for i in range(0,n_layer-1):\n",
    "        tmp = dense_layer[i]*mask[i]\n",
    "        if(i==n_layer-2):\n",
    "            tmp*=mu_mask\n",
    "        ys=linear_nonlinear(tmp,ys)\n",
    "    #print(\"ys\",ys.shape)\n",
    "    fs = torch.sum(ys,dim=1)\n",
    "    #print(\"fs\",fs.shape)\n",
    "    dfs = []\n",
    "    for i in range(n_conv):\n",
    "        tmp = grad(fs,ns[i],grad_outputs=torch.ones_like(fs),create_graph=True)[0]\n",
    "        dfs.append(tmp)\n",
    "    dfs = torch.stack(dfs,dim=0)\n",
    "    #print(\"dfs\",dfs.shape)\n",
    "    dfs_conv=[]\n",
    "    for i in range(n_conv):\n",
    "        tmp = simple_conv(dfs[i],w_torch[i],cross=True)\n",
    "        #tmp.retain_grad()\n",
    "        dfs_conv.append(tmp)  \n",
    "    dfs_conv = torch.stack(dfs_conv)\n",
    "    c1=torch.sum(dfs_conv,dim=0)\n",
    "    rhoMLs=torch.exp(-c1-Vexts)\n",
    "    muMLs=cal_mu(rhos,rhoMLs)\n",
    "    rhoMLs = rhoMLs*torch.unsqueeze(torch.exp(muMLs),1)\n",
    "    \n",
    "    w_real = []\n",
    "    for w in w_torch:\n",
    "        tmp = torch.stack([w[0],w[1]],dim=-1)\n",
    "        tmp=torch.irfft(tmp,1,signal_sizes=rhos[0].shape)\n",
    "        #print(tmp.shape)\n",
    "        tmp=torch.nn.functional.pad(tmp,pad=(0, 1,), mode='constant', value=0)\n",
    "        tmp[-1]=tmp[0]\n",
    "        w_real.append(tmp)\n",
    "        \n",
    "    w_real=torch.stack(w_real)\n",
    "    abs_w_real = torch.abs(w_real)\n",
    "    w_sym = torch.abs(abs_w_real-torch.flip(abs_w_real,dims=[1]))\n",
    "    w_bound = torch.abs(w_tanh_bound*abs_w_real)\n",
    "        \n",
    "    return rhoMLs,muMLs,torch.mean(w_sym+w_bound)+torch.mean(torch.abs(w_torch[:,1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = FEQL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model():\n",
    "    final_fed = final_equation()\n",
    "    filename = 'final_eq.dat'\n",
    "    outfile = open(filename,'wb')\n",
    "    pickle.dump(final_fed, outfile)\n",
    "    outfile.close()\n",
    "    \n",
    "    filename = 'w_weight'\n",
    "    outfile = open(filename,'wb')\n",
    "    pickle.dump(w_torch.cpu().detach().numpy(),outfile)\n",
    "    outfile.close()\n",
    "    \n",
    "    filename = 'dense_layer'\n",
    "    outfile = open(filename,'wb')\n",
    "    pickle.dump([dense_layer_0.cpu().detach().numpy(),dense_layer.cpu().detach().numpy()],outfile)\n",
    "    outfile.close()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load or initial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    filename = 'w_weight'\n",
    "    outfile = open(filename,'rb')\n",
    "    w_torch_ini=pickle.load(outfile)\n",
    "    outfile.close()\n",
    "\n",
    "    filename = 'dense_layer'\n",
    "    outfile = open(filename,'rb')\n",
    "    [dense_layer_0_ini,dense_layer_ini]=pickle.load(outfile)\n",
    "    outfile.close()\n",
    "    return w_torch_ini,dense_layer_0_ini,dense_layer_ini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7efbc0975e50>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(0,L,N+1)\n",
    "d= 4 \n",
    "w = 1\n",
    "w_tanh_bound = torch.tensor(np.tanh((x-d)/w)-np.tanh((x-L+d)/w), device=device)\n",
    "plt.plot(x,w_tanh_bound.cpu())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0,w1=w_FMT()\n",
    "w_torch_ini=np.random.randn(n_conv,2,conv_dim)*0.001\n",
    "w_torch_ini[0] = [np.real(w0)/10,np.imag(w0)]  \n",
    "w_torch_ini[1] = [np.real(w1)/10,np.imag(w1)]\n",
    "\n",
    "[dense_layer_0_ini,dense_layer_ini]=[np.random.randn(n_dim1,n_conv)*0.001,\n",
    "                                     np.random.randn(n_layer-1,n_dim1,n_dim1p)*0.001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_weight = False\n",
    "if(load_weight==True):\n",
    "    w_torch_ini,dense_layer_0_ini,dense_layer_ini=load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "w_torch = torch.tensor(w_torch_ini, requires_grad=True, device=device)\n",
    "\n",
    "dense_layer_0=torch.tensor(dense_layer_0_ini, dtype=torch.float64, requires_grad=True, device=device)\n",
    "dense_layer=torch.tensor(dense_layer_ini, dtype=torch.float64, requires_grad=True, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "kill=0\n",
    "fed = final_equation()\n",
    "fed;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "log=open(\"log_loss.dat\", \"w\")\n",
    "log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time  6.817176818847656 s  \tepoch= 0 / 1000 \t loss =  0.3832187137723305 \t val loss =  0.3864671088487559\n"
     ]
    }
   ],
   "source": [
    "alpha1=0.9\n",
    "alpha2=0.1\n",
    "#asym = 10**-3\n",
    "\n",
    "for phase in range (1,4):\n",
    "    if(phase==1):\n",
    "        conv_penalty=0\n",
    "        dense_penalty=0\n",
    "        kill = 0\n",
    "        n_epochs=1000\n",
    "        lr=10**-3\n",
    "        asym=10**-5\n",
    "    \n",
    "    if(phase==2):\n",
    "        conv_penalty=0\n",
    "        dense_penalty=10**-3\n",
    "        kill = 0.05\n",
    "        n_epochs=20000\n",
    "        lr=10**-3\n",
    "        asym=10**-3\n",
    "        \n",
    "    if(phase==3):\n",
    "        conv_penalty=0\n",
    "        dense_penalty=0\n",
    "        kill = 0.1\n",
    "        n_epochs=1000\n",
    "        lr=10**-3\n",
    "        asym=10**-3\n",
    "        \n",
    "    MAE = torch.nn.L1Loss()\n",
    "    optimizer = torch.optim.Adam([w_torch,dense_layer_0,dense_layer],lr=lr)\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(n_epochs+1):\n",
    "        start=time.time()\n",
    "        loss_batch=[]\n",
    "        for rho_batch, Vext_batch,mu_batch in train_loader:\n",
    "            rho_batch=rho_batch.to(device)\n",
    "            Vext_batch=Vext_batch.to(device)\n",
    "            mu_batch=mu_batch.to(device)\n",
    "            rhoMLs,muMLs,w_sym=FEQL(rho_batch,Vext_batch,kill)\n",
    "            loss = MAE(rhoMLs,rho_batch)+alpha2*MAE(muMLs,mu_batch)+\\\n",
    "                    conv_penalty*MAE(w_torch,torch.zeros_like(w_torch))+asym*w_sym+\\\n",
    "                    dense_penalty*(MAE(dense_layer_0,torch.zeros_like(dense_layer_0))+\n",
    "                                   MAE(dense_layer,torch.zeros_like(dense_layer)))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            loss_batch.append(loss.data.cpu().numpy())\n",
    "        train_losses.append(np.mean(loss_batch))\n",
    "        #save_model()\n",
    "        loss_batch=[]\n",
    "        for rho_batch,Vext_batch,mu_batch in val_loader:\n",
    "            rho_batch=rho_batch.to(device)\n",
    "            Vext_batch=Vext_batch.to(device)\n",
    "            mu_batch=mu_batch.to(device)\n",
    "            rhoMLs,muMLs,w_sym = FEQL(rho_batch,Vext_batch,kill)\n",
    "            loss = MAE(rhoMLs,rho_batch)+alpha2*MAE(muMLs,mu_batch)+\\\n",
    "                    conv_penalty*MAE(w_torch,torch.zeros_like(w_torch))+asym*w_sym+\\\n",
    "                    dense_penalty*(MAE(dense_layer_0,torch.zeros_like(dense_layer_0))+\n",
    "                                   MAE(dense_layer,torch.zeros_like(dense_layer)))\n",
    "            loss_batch.append(loss.data.cpu().numpy())\n",
    "        val_losses.append(np.mean(loss_batch))\n",
    "        end=time.time()\n",
    "        log=open(\"log_loss.dat\", \"a\")\n",
    "        log.write(\"loss = \"+str(train_losses[-1])+\",\\t val loss = \"+str(val_losses[-1])+\"\\n\")\n",
    "        log.close()\n",
    "        save_model()\n",
    "        print(\"time \",end-start,\"s \",\"\\tepoch=\",epoch,\"/\",n_epochs,\"\\t loss = \",train_losses[-1],\"\\t val loss = \",val_losses[-1])\n",
    "    save_model()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'final_eq.dat'\n",
    "outfile = open(filename,'rb')\n",
    "final_fed = pickle.load(outfile)\n",
    "outfile.close()\n",
    "final_fed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_w_to_np(w):\n",
    "    w_res = w.data.cpu().numpy()\n",
    "    w_res = (w_res[0]+w_res[1]*1j)\n",
    "    return w_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1_result = convert_w_to_np(w_torch[1])\n",
    "plt.plot(np.fft.irfft(w_torch_ini[0][0]+w_torch_ini[0][1]*1j),\"b-\")\n",
    "plt.plot(np.fft.irfft(w1_result),\"k--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1_result = convert_w_to_np(w_torch[1])\n",
    "plt.plot(np.fft.irfft(w_torch_ini[0][0]+w_torch_ini[0][1]*1j)-np.fft.irfft(w1_result),\"b-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(w_torch_ini[1][0]-np.real(w1_result),\"b-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.ones(1024)*2\n",
    "b=torch.ones(2,1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
