{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    " \n",
    "# The GPU id to use, usually either \"0\" or \"1\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\";  \n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "# Do other imports now...\n",
    "\n",
    "from scipy.integrate import simps\n",
    "from numpy import exp, absolute\n",
    "from numpy import exp, asarray, empty\n",
    "from numpy import zeros, array, float, random\n",
    "import itertools as it\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.layers import Input, Dense,Conv1D,Reshape,Flatten\n",
    "from keras.layers import Add,Subtract,Multiply\n",
    "from keras.layers import Lambda,RepeatVector\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.models import model_from_json\n",
    "from keras import backend as K \n",
    "from keras.optimizers import Adam,RMSprop,SGD\n",
    "from keras.initializers import RandomNormal,Constant\n",
    "from keras import regularizers\n",
    "from keras.layers import Layer\n",
    "from keras.callbacks import ModelCheckpoint,LambdaCallback,Callback,EarlyStopping\n",
    "K.clear_session()\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from Custom_layers import conv_pbc,variable,const,kill_small,weight_kernel#self_defined_layers\n",
    "\n",
    "\n",
    "#from equation_gen_new import equation_gen_couple_eps_non_bias  #Full \n",
    "#from equation_gen_new import equation_gen_non_couple_non_bias\n",
    "#from equation_gen_new import equation_gen_couple_HR_non_bias   #only athermal term\n",
    "from equation_gen_new import equation_gen_sparse #sparse\n",
    "from sympy import diff,log,symbols,exp,simplify\n",
    "from sympy.utilities.lambdify import lambdify\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = '../LJ_data_all_correct/'\n",
    "f=np.loadtxt(data_file+\"MC_parameter.dat\")\n",
    "L = float(f[0])\n",
    "dx = float(f[1])\n",
    "N = int(L/dx)\n",
    "print(N,L,dx)\n",
    "input_shape = (N,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(data_file+'/MC_inform.dat', 'r')\n",
    "MC_inform = f.read().splitlines()\n",
    "for i in range(len(MC_inform)):\n",
    "    #print(MC_inform[i])\n",
    "    MC_inform[i]=MC_inform[i].split(\"\\t\")\n",
    "f.close()\n",
    "#MC_inform = np.asarray(MC_inform)\n",
    "batch_size = len(MC_inform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w_FMT(eps):\n",
    "    #d=cal_eff_diameter (eps)\n",
    "    R = 1.0/2\n",
    "    k=np.linspace(0,N//2,N//2+1)*2*np.pi/L\n",
    "    w0=2*np.cos(k*R)/2\n",
    "    k[0]=1 #keep notebook shutup\n",
    "    w1=2*np.sin(k*R)/k\n",
    "    w1[0]=2*R\n",
    "    return w0,w1;\n",
    "\n",
    "def cal_n(rho,w):\n",
    "    return np.fft.irfft(np.fft.rfft(rho)*w)\n",
    "\n",
    "def cal_c1_FMT(rho,eps=0):\n",
    "    w0,w1 = w_FMT(eps)\n",
    "    n0=cal_n(rho,w0)\n",
    "    n1=cal_n(rho,w1)\n",
    "    F0=-np.log(1-n1)\n",
    "    F1=n0/(1-n1)\n",
    "    #print(np.min(1-n1))\n",
    "    return cal_n(F0,w0)+cal_n(F1,w1)\n",
    "    #return np.zeros(len(rho))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MC_inform[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho= []\n",
    "c1_HR = []\n",
    "Vext= []\n",
    "mu = []\n",
    "eps= []\n",
    "\n",
    "for i in range (batch_size):\n",
    "    Vext +=[np.loadtxt(data_file+\"Vext_\"+str(i)+\".dat\")]\n",
    "    rho += [np.loadtxt(data_file+\"rho_\"+str(i)+\".dat\")]\n",
    "    c1_HR+=[cal_c1_FMT(rho[i])]\n",
    "    mu += [np.log(np.float(MC_inform[i][2]))]\n",
    "    eps += [np.float(MC_inform[i][1])]\n",
    "for i in range (batch_size):\n",
    "    Vext +=[np.flip(\n",
    "        np.loadtxt(data_file+\"Vext_\"+str(i)+\".dat\"))]\n",
    "    rho += [np.flip(\n",
    "        np.loadtxt(data_file+\"rho_\"+str(i)+\".dat\"))]\n",
    "    c1_HR+=[cal_c1_FMT(rho[batch_size+i])]\n",
    "    mu += [np.log(np.float(MC_inform[i][2]))]\n",
    "    eps += [np.float(MC_inform[i][1])]\n",
    "    \n",
    "#inputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(eps),np.max(eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(np.min(mu)),np.exp(np.max(mu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(c1_HR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.exp(mu[0]-c1_HR[0]-Vext[0]))\n",
    "plt.plot(rho[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho_train=np.asarray(rho)\n",
    "rho_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho_train=np.asarray(rho)\n",
    "c1_HR_train=np.asarray(c1_HR)\n",
    "Vext_train=np.asarray(Vext)\n",
    "eps_train = np.asarray(eps)\n",
    "mu_train = np.asarray(mu)\n",
    "deltaF_train = np.asarray(np.zeros(rho_train.shape))\n",
    "\n",
    "\n",
    "rho_train = rho_train.reshape(rho_train.shape[0], N , 1)\n",
    "c1_HR_train = c1_HR_train.reshape(c1_HR_train.shape[0], N , 1)\n",
    "Vext_train = Vext_train.reshape(Vext_train.shape[0], N , 1)\n",
    "eps_train = eps_train.reshape(eps_train.shape[0], 1 , 1)\n",
    "mu_train = mu_train.reshape(mu_train.shape[0], 1 , 1)\n",
    "deltaF_train=deltaF_train.reshape(deltaF_train.shape[0], N , 1)\n",
    "\n",
    "test_size=0.1\n",
    "\n",
    "rho_train, rho_test ,c1_HR_train, c1_HR_test,Vext_train, Vext_test,eps_train,\\\n",
    "eps_test, mu_train, mu_test, deltaF_train, deltaF_test\\\n",
    "= train_test_split(rho_train,c1_HR_train,Vext_train,eps_train,mu_train, deltaF_train , test_size=test_size)\n",
    "#c1_HR_train, c1_HR_test    = train_test_split(c1_HR_train, test_size=test_size)\n",
    "#Vext_train, Vext_test= train_test_split(Vext_train, test_size=test_size)\n",
    "#eps_train, eps_test  = train_test_split(eps_train, test_size=test_size)\n",
    "#mu_train, mu_test    = train_test_split(mu_train, test_size=test_size)\n",
    "\n",
    "\n",
    "print(rho_train.shape)\n",
    "print(rho_test.shape)\n",
    "print(c1_HR_train.shape)\n",
    "print(Vext_train.shape)\n",
    "print(mu_train.shape)\n",
    "print(eps_train.shape)\n",
    "print(deltaF_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rho_train[0],label=\"rho1\")\n",
    "plt.plot(Vext_train[0],label=\"V1\")\n",
    "\n",
    "plt.plot(rho_test[0],label=\"rho2\")\n",
    "plt.plot(Vext_test[0],label=\"V2\")\n",
    "plt.legend()\n",
    "plt.ylim([-2,5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho_train[0];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate free energy density on the fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layer=3\n",
    "n_density=3\n",
    "n_density_eps=1\n",
    "n_parameter=1\n",
    "n_id = 1\n",
    "n_log = 1\n",
    "n_exp = 1\n",
    "n_mul = 2\n",
    "n_div = 1\n",
    "\n",
    "fed = equation_gen_sparse(n_layer,n_density,n_parameter,n_id,n_log,n_exp,n_mul,n_div,n_density_eps)\n",
    "# no bias is dangerous for log, non-couple may have particle density not coupled with others "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fed.subs('eps0',0)\n",
    "fed.subs('eps0',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save all symbols to all_symbols.dat\n",
    "name = \"all_symbols.dat\"\n",
    "with open(name, \"w\") as text_file:\n",
    "    text_file.write(str(fed.free_symbols))\n",
    "tmp = open(name, \"r\")\n",
    "syms= tmp.read()\n",
    "tmp.close()\n",
    "syms = syms.replace(\"{\", \"\")\n",
    "syms = syms.replace(\"}\", \"\")\n",
    "syms = syms.replace(\" \", \"\")\n",
    "syms = syms.split(',')\n",
    "syms=sorted(tuple(syms))\n",
    "syms2 = ['s' + i for i in syms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syms2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial all symbols and syms\n",
    "name = \"assign_symbols.dat\"\n",
    "with open(name, \"w\") as text_file:\n",
    "    text_file.write(\"ini\")\n",
    "    text_file.write(str(syms2)+\"=symbols(\"+str(syms)+\")\\n\")\n",
    "    text_file.write(\"syms=\"+str(syms2))\n",
    "tmp = open(name, \"r\")\n",
    "tmp=tmp.read()\n",
    "tmp=tmp.replace(\"'\", \"\")\n",
    "tmp=tmp.replace(\"[\", \"(\")\n",
    "tmp=tmp.replace(\"]\", \")\")\n",
    "tmp=tmp.replace(\"((\", \"('\")\n",
    "tmp=tmp.replace(\"))\", \"')\")\n",
    "tmp=tmp.replace(\"ini(\", \"\")\n",
    "tmp=tmp.replace(\")=symbols\", \"=symbols\")\n",
    "exec(tmp)\n",
    "name = \"assign_symbols_final.dat\"\n",
    "with open(name, \"w\") as text_file:\n",
    "    text_file.write(tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def fed_eqn\n",
    "fed_string = str(fed)\n",
    "fed_string=fed_string.replace(\"a\", \"sa\")\n",
    "fed_string=fed_string.replace(\"b\", \"sb\")\n",
    "fed_string=fed_string.replace(\"n\", \"sn\")\n",
    "fed_string=fed_string.replace(\"eps\", \"seps\")\n",
    "\n",
    "name = \"free_energy_final.dat\"\n",
    "with open(name, \"w\") as text_file:\n",
    "    text_file.write(fed_string)\n",
    "\n",
    "name = \"free_energy_def.dat\"\n",
    "with open(name, \"w\") as text_file:\n",
    "    text_file.write(\"def fed_eqn():\\n\")\n",
    "    text_file.write(\"     return \"+fed_string)\n",
    "tmp = open(name, \"r\")\n",
    "tmp=tmp.read()\n",
    "exec(tmp)\n",
    "\n",
    "    \n",
    "fed_tf= lambdify([syms],fed_eqn(),'tensorflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(syms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_conv = (n_density)+(n_parameter*n_density_eps) #(n_parameter+1) for full "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"diff_equations.dat\"\n",
    "with open(name, \"w\") as text_file:\n",
    "    for i in range (n_conv):\n",
    "        text_file.write(\"def Df_n\"+str(i)+\"():\\n\")\n",
    "        text_file.write(\"     return diff(fed_eqn(),sn\"+str(i)+\")\\n\")\n",
    "        text_file.write(\"\\n\")\n",
    "        text_file.write(\"df_n\"+str(i)+\"= lambdify([syms],Df_n\"+str(i)+\"(),\\\"tensorflow\\\")\\n\") \n",
    "        text_file.write(\"\\n\")\n",
    "        \n",
    "tmp = open(name, \"r\")\n",
    "tmp=tmp.read()\n",
    "exec(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_str = \"({\"\n",
    "for i in range (n_conv):\n",
    "    sub_str+=\"\\\"n\"+str(i)+\"\\\":0\"\n",
    "    if(i!=n_conv-1):\n",
    "        sub_str+=\",\"\n",
    "sub_str += \"})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gen_s3=\"s3=(\"\n",
    "for i in range (n_conv):\n",
    "    gen_s3+=\"+diff(fed_eqn(),sn\"+str(i)+\").subs\"+sub_str\n",
    "gen_s3+=\").free_symbols\"\n",
    "print(gen_s3)\n",
    "exec(gen_s3)\n",
    "print(s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp=str(s3)\n",
    "tmp=tmp.replace(\"'\", \"\")\n",
    "tmp=tmp.replace(\" \", \"\")\n",
    "tmp=tmp.replace(\"{\", \"\")\n",
    "tmp=tmp.replace(\"}\", \"\")\n",
    "tmp = tuple(tmp.split(','))\n",
    "tmp = sorted(tmp)\n",
    "print(tmp)\n",
    "redunt = \"\"\n",
    "\n",
    "for i in range(len(tmp)):\n",
    "    if tmp[i][1].isdigit():\n",
    "        if int(tmp[i][1])>1:\n",
    "            redunt+=\"s\"+tmp[i]+\"+\"\n",
    "redunt+=\"0\"  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(redunt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redunt_eq=eval(redunt)\n",
    "s3=redunt_eq.free_symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1=fed_eqn().free_symbols\n",
    "s1-s3;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1=fed_eqn().free_symbols\n",
    "\n",
    "gen_s2=\"s2=(\"\n",
    "for i in range (n_conv):\n",
    "    gen_s2+=\"+diff(fed_eqn(),sn\"+str(i)+\")\"\n",
    "gen_s2+=\").free_symbols\"\n",
    "#print(gen_s2)\n",
    "exec(gen_s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redunt = str(s3)\n",
    "redunt=redunt.replace(\"'\", \"\")\n",
    "redunt=redunt.replace(\" \", \"\")\n",
    "redunt=redunt.replace(\"{\", \"\")\n",
    "redunt=redunt.replace(\"}\", \"\")\n",
    "redunt = tuple(redunt.split(','))\n",
    "redunt = sorted(redunt)\n",
    "\n",
    "train_val = str(s2-s3)\n",
    "train_val=train_val.replace(\"'\", \"\")\n",
    "train_val=train_val.replace(\" \", \"\")\n",
    "train_val=train_val.replace(\"{\", \"\")\n",
    "train_val=train_val.replace(\"}\", \"\")\n",
    "train_val= tuple(train_val.split(','))\n",
    "train_val = sorted(train_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"redudent.dat\"\n",
    "with open(name, \"w\") as text_file:\n",
    "    text_file.write(str(redunt))\n",
    "\n",
    "name = \"trainable.dat\"\n",
    "with open(name, \"w\") as text_file:\n",
    "    text_file.write(str(train_val))\n",
    "    \n",
    "print(len(s1),len(s2),len(s3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate model on the fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_dim = int((int(16/dx/2))*2+1)\n",
    "conv_h = int((conv_dim-1)/2) # half of conv_dim\n",
    "print(conv_dim,conv_h,conv_dim*dx)\n",
    "\n",
    "K.clear_session()\n",
    "np.random.seed(424242)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_z(s): #particle N conserved\n",
    "    [rho,rhoML]=s\n",
    "    return K.log(K.sum(rho,axis=1,keepdims=True)/K.sum(rhoML,axis=1,keepdims=True))\n",
    "\n",
    "def cal2_z(s): #(rho-z*rhoML)**2 mini\n",
    "    [rho,rhoML]=s\n",
    "    #res = K.sum(rho[1]*rhoML[1],axis=1)/K.sum(rhoML[1]*rhoML[1],axis=1)\n",
    "    res1=K.sum(rho*rhoML,axis=1,keepdims=True)\n",
    "    res2=K.sum(rhoML*rhoML,axis=1,keepdims=True)\n",
    "    #print(res1.shape,len(res1.shape))\n",
    "    if(len(res1.shape)>3):\n",
    "        for i in range (2,len(res1.shape)-1):\n",
    "            res1=K.sum(res1,axis=i,keepdims=True)\n",
    "            res2=K.sum(res2,axis=i,keepdims=True)\n",
    "    #res=K.sum(rho*rhoML,axis=1,keepdims=True)/K.sum(rhoML*rhoML,axis=1,keepdims=True)\n",
    "    #print(res.shape)\n",
    "    return K.log(res1/res2)\n",
    "\n",
    "def pbc_cross(s):\n",
    "    a,w=s\n",
    "    print(\"cross\")\n",
    "    a = K.tile(a,[1,3,1])\n",
    "    a = a[:,N-conv_h:2*N+conv_h,:]\n",
    "    a = K.conv1d(a,w, padding='valid')\n",
    "    a = a*dx\n",
    "    return a\n",
    "\n",
    "def pbc_conv(s):\n",
    "    a,w=s\n",
    "    print(\"conv\")\n",
    "    a = K.tile(a,[1,3,1])\n",
    "    a = a[:,N-conv_h:2*N+conv_h,:]\n",
    "    a = K.conv1d(a,K.reverse(w,axes=0), padding='valid')\n",
    "    #a = K.reverse(a,axes=1)\n",
    "    a = a*dx\n",
    "    return a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"The_model.dat\"\n",
    "with open(model_name, \"w\") as file:\n",
    "    file.write(\"def encoder(penalty,kill,conv_penalty):\\n\")\n",
    "    \n",
    "    file.write(\"    rho = Input(shape=input_shape)\\n\")\n",
    "    file.write(\"    c1_HR = Input(shape=input_shape)\\n\")\n",
    "    file.write(\"    Vext = Input(shape=input_shape)\\n\")\n",
    "    for i in range (n_parameter):\n",
    "        file.write(\"    eps\"+str(i)+\" = Input(shape=(1,1))\\n\")\n",
    "    file.write(\"    mu = Input(shape=(1,1))\\n\")\n",
    "    \n",
    "    file.write(\"\\n\")\n",
    "    file.write(\"    dummy = mu\\n\")\n",
    "    for i in range (n_conv):\n",
    "        file.write(\"    w\"+str(i)+\" = weight_kernel(conv_dim,mean=(np.random.rand()-0.5)*0.02,penalty=conv_penalty,name=\\\"w\"+str(i)+\"\\\")(dummy)\\n\")    \n",
    "    #for i in range (n_density):\n",
    "    for i in range (n_conv):\n",
    "        file.write(\"    n\"+str(i)+\" = Lambda(pbc_conv)([rho,w\"+str(i)+\"])\\n\")\n",
    "    file.write(\"\\n\")           \n",
    "    \n",
    "    for i in range(len(train_val)):\n",
    "        if(str(train_val[i])[0:3]!=\"eps\" and str(train_val[i])[0:1]!=\"n\"):\n",
    "            if(str(train_val[i])[0]==\"a\"):\n",
    "                file.write(\"    \"+str(train_val[i])+\"=variable((np.random.rand()-0.5)*0.02,penalty\"+\",name=\\\"\"+str(train_val[i])+\"\\\")(dummy)\\n\") \n",
    "            elif(str(train_val[i])[0]==\"b\"):\n",
    "                file.write(\"    \"+str(train_val[i])+\"=variable(0.1,penalty\"+\",name=\\\"\"+str(train_val[i])+\"\\\")(dummy)\\n\") ## ini some number not zero for safety\n",
    "            \n",
    "    file.write(\"\\n\")\n",
    "    \n",
    "    for i in range(len(redunt)):\n",
    "        if(str(redunt[i])[0]==\"a\" or str(redunt[i])[0]==\"b\"):\n",
    "            file.write(\"    \"+str(redunt[i])+\"=const(name=\\\"\"+str(redunt[i])+\"\\\")(dummy)\\n\") ## non trainble , just set to zero\n",
    "    file.write(\"\\n\")\n",
    "    \n",
    "    #file.write(\"    kill=const(\"+str(kill)+\")\\n\")\n",
    "    for i in range(len(train_val)):\n",
    "        #if(str(train_val[i])[0]==\"a\" or str(train_val[i])[0]==\"b\"):\n",
    "        if(str(train_val[i])[0]==\"a\"):    \n",
    "            file.write(\"    \"+str(train_val[i])+ \"=kill_small(kill)(\"+str(train_val[i])+\")\\n\")\n",
    "    file.write(\"\\n\")\n",
    "    \n",
    "    file.write(\"    s = [\"+str(syms)[1:-1]+\"]\\n\")\n",
    "    file.write(\"\\n\")\n",
    "    \n",
    "    for i in range (n_conv):\n",
    "        file.write(\"    f\"+str(i)+\" = Lambda(lambda x: df_n\"+str(i)+\"(x))(s)\\n\")\n",
    "        file.write(\"    f\"+str(i)+\" = Lambda(pbc_cross)([f\"+str(i)+\",\"+\"w\"+str(i)+\"])\\n\")\n",
    "    file.write(\"\\n\")\n",
    "    file.write(\"    c1 = Add()([\")\n",
    "    for i in range (n_conv):\n",
    "        file.write(\"f\"+str(i))\n",
    "        if(i!=n_conv-1):\n",
    "            file.write(\",\")\n",
    "    file.write(\"])\")\n",
    "    file.write(\"\\n\")\n",
    "    file.write(\"    print(c1.shape)\\n\")\n",
    "    file.write(\"    print(eps0.shape)\\n\")\n",
    "    file.write(\"    c1=keras.layers.Multiply()([c1,eps0])\\n\")\n",
    "    file.write(\"    print(c1.shape)\\n\")\n",
    "    \n",
    "    file.write(\"    rhoML = Lambda(lambda x: K.exp(-x[0]-x[1]-x[2]))([c1,c1_HR,Vext])\\n\")\n",
    "    file.write(\"    muML = Lambda(cal2_z)([rho,rhoML])# dont change order of muML and rhoML\\n\")  \n",
    "    #file.write(\"    muML = Lambda(cal_z)([rho,rhoML])# dont change order of muML and rhoML\\n\")  \n",
    "    file.write(\"    rhoML= Lambda(lambda x: K.exp(x[0])*x[1])([muML,rhoML])\\n\")\n",
    "\n",
    "    for i in range (n_conv):\n",
    "        file.write(\"    asymw\"+str(i)+\" = Lambda(lambda x:dx*K.sum(K.abs(K.abs(x)-K.abs(K.reverse(x,axes=0))),axis=0,keepdims=True))(w\"+str(i)+\")\\n\")\n",
    "    file.write(\"    asym = Add()([\")\n",
    "    for i in range (n_conv):\n",
    "        file.write(\"asymw\"+str(i))\n",
    "        if(i!=n_conv-1):\n",
    "            file.write(\",\")\n",
    "    file.write(\"])\")\n",
    "    file.write(\"\\n\")\n",
    "    file.write(\"    print(asym.shape)\\n\")\n",
    "    file.write(\"    asym= Lambda(lambda x:K.abs(x[0])/K.abs(x[0])*x[1])([mu,asym])\\n\")\n",
    "    file.write(\"    print(asym.shape)\\n\")\n",
    "    \n",
    "    \n",
    "    file.write(\"    model = Model([rho,c1_HR,Vext\")\n",
    "    for i in range (n_parameter):\n",
    "        file.write(\",eps\"+str(i))         \n",
    "    file.write(\",mu], [rhoML,muML,asym] , name = \\\"F_ML\\\")\\n\")\n",
    "    #file.write(\"    model.summary()\\n\")\n",
    "    file.write(\"\\n\")\n",
    "    file.write(\"    return model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = open(model_name, \"r\")\n",
    "tmp=tmp.read()\n",
    "exec(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#F_ML.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## choo choo ~ (training part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logname=\"log_file.dat\"\n",
    "with open(logname, \"w\") as file:\n",
    "    file.write(\"\\n\")\n",
    "def show_loss(epoch, logs):\n",
    "    if(epoch%50==0):\n",
    "        print(\"epcoh\\t=\\t\"+str(epoch)+\"\\t\"+str(logs))\n",
    "    logname=\"log_file.dat\"\n",
    "    with open(logname, \"a\") as file:\n",
    "        file.write(str(logs)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoppingByLossNan(Callback):\n",
    "    def __init__(self, monitor='val_loss',monitor2='loss', verbose=0):\n",
    "        super(Callback, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.monitor2 = monitor2\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        current = logs.get(self.monitor)\n",
    "        current2 = logs.get(self.monitor2)\n",
    "        if(current is None or current2 is None):\n",
    "            warnings.warn(\"Early stopping requires %s available!\" % self.monitor, RuntimeWarning)\n",
    "            warnings.warn(\"Early stopping requires %s available!\" % self.monitor2, RuntimeWarning)\n",
    "\n",
    "        if(np.isnan(current) or np.isinf(current) or np.isnan(current2) or np.isinf(current2)):\n",
    "            print(\"loss diverge at epoch = \" + str(epoch))\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"F_ML.hdf5\"\n",
    "checkpoint=ModelCheckpoint(file, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='min', period=1)\n",
    "stop=EarlyStopping(monitor='val_loss', min_delta=0, patience=1000,  \n",
    "                   verbose=0, mode='min', baseline=None, restore_best_weights=True)\n",
    "epochLogCallback = LambdaCallback(on_epoch_end=show_loss)\n",
    "\n",
    "callbacks_list = [checkpoint,epochLogCallback,StoppingByLossNan(monitor='val_loss',monitor2='loss', verbose=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_parameters(weight_dict,kill):\n",
    "    w_array = np.zeros([n_conv,conv_dim])\n",
    "    for i in range (n_conv):\n",
    "        for j in list(weight_dict.keys()):\n",
    "            arr = weight_dict[j][1]\n",
    "            if(n_conv<10):\n",
    "                if(j[0]==\"w\" and j[1]==str(i)):\n",
    "                    w_array[i]= np.copy(arr.flatten())\n",
    "            elif(n_conv<100):\n",
    "                if(j[0]==\"w\" and j[1:3]==str(i)):\n",
    "                    w_array[i]= np.copy(arr.flatten())\n",
    "            else:\n",
    "                print(\"n>100? serious?\")\n",
    "    np.savetxt(\"w_array.dat\",w_array)\n",
    "\n",
    "    name=\"all_parameter.dat\"        \n",
    "    with open(name, \"w\") as file:\n",
    "        for i in list(weight_dict.keys()):\n",
    "            arr = weight_dict[i][1]\n",
    "            arr = np.array(arr.flatten())\n",
    "            #arr = np.roll(arr,conv_h)\n",
    "            if(i[0]!=\"w\"):\n",
    "                file.write(str(i)+\" \"+str(weight_dict[i][1])+\"\\n\")\n",
    "    name = \"ML_parameter.dat\"\n",
    "    with open(name, \"w\") as text_file:\n",
    "        text_file.write(\"n_parameter=\"+str(n_parameter)+\"\\n\")\n",
    "        text_file.write(\"n_conv=\"+str(n_conv)+\"\\n\")\n",
    "        text_file.write(\"conv_dim=\"+str(conv_dim)+\"\\n\")\n",
    "        text_file.write(\"kill=\"+str(kill)+\"\\n\")\n",
    "\n",
    "\n",
    "    print(\"output_parameters\")\n",
    "    return w_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for phase in range (0,3):\n",
    "    penalty = 0#0.00001\n",
    "    conv_penalty=0\n",
    "    epochs=5000\n",
    "    kill = 0\n",
    "    asym = 10**-4\n",
    "    lr = 10**-3\n",
    "    print(\"phase = \"+str(phase))\n",
    "    if(phase==1):\n",
    "        penalty = 8*10**-5\n",
    "        conv_penalty=10**-7\n",
    "        kill = 0\n",
    "        epochs=5000\n",
    "        asym=10**-6\n",
    "        lr = 10**-3\n",
    "    if(phase==2):\n",
    "        penalty = 0\n",
    "        conv_penalty= 0\n",
    "        kill = 0.05\n",
    "        epochs=1000\n",
    "        asym=0\n",
    "        lr = 10**-3\n",
    "    \n",
    "    x1 = Input(shape=input_shape)\n",
    "    x2 = Input(shape=input_shape)\n",
    "    x3 = Input(shape=input_shape)\n",
    "    x4 = Input(shape=(1,1))\n",
    "    x5 = Input(shape=(1,1))\n",
    "\n",
    "    build = encoder(penalty,kill,conv_penalty)\n",
    "\n",
    "    #build.\n",
    "    [y1,y2,y3] = build([x1,x2,x3,x4,x5])\n",
    "\n",
    "    F_ML = Model([x1,x2,x3,x4,x5],[y1,y2,y3], name='F_learn')\n",
    "    optimal = 'mae'\n",
    "    F_ML.compile(optimizer=Adam(lr=lr), loss=optimal,loss_weights=[0.9, 0.1,asym]) #lr=0.00002\n",
    "    if(os.path.isfile(file)):\n",
    "        if(phase !=0 ):\n",
    "            F_ML.load_weights(file)\n",
    "    F_ML.fit(x=[rho_train,c1_HR_train,Vext_train,eps_train,mu_train], \n",
    "             y=[rho_train,mu_train,mu_train*0],\n",
    "             epochs=epochs,\n",
    "             shuffle=True,\n",
    "             batch_size=64,\n",
    "             validation_data=([rho_test,c1_HR_test,Vext_test,eps_test,mu_test], \n",
    "                              [rho_test,mu_test,mu_test*0]),\n",
    "             callbacks=callbacks_list,\n",
    "             verbose=2\n",
    "            )\n",
    "    F_ML.load_weights(file)\n",
    "\n",
    "    names = [weight.name for layer in F_ML.layers for weight in layer.weights]\n",
    "    weights = F_ML.get_weights()\n",
    "    weight_dict = {}\n",
    "    for name, weight in zip(names, weights):\n",
    "        #print(name)\n",
    "        weight_dict[str(name)]=[weight.shape,weight]\n",
    "    w_array=output_parameters(weight_dict,kill)\n",
    "    copyfile(file, \"meta_phase_\"+str(phase)+\".h5\")\n",
    "    print(\"lr=\"+str(lr)+\"\\tstep\\t=\\t\"+str(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
