This repository is for learnining  functionals from an equation learning network (
see my paper "Analytical classical density functionals from an equation learning network" for details, 
arXiv:1910.12752  paper:soon). 
The main idea is combining "equation learner" (https://github.com/martius-lab) and convolutional neuron network to 
learn functionals, and apply to classical density functional theory problems. 

The main libraries are tensorflow-gpu 1.12 and sympy 1.3. Nvidia GPU and cuda are strongly recommend. 
The explicity library list is in "tf_file.txt". 
One can use conda to creat exactly same enviroment easily (conda create --name name_of_env --file tf_file.txt  , 
see https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html for more details).

XXX.ipynb runs with jupyter-notebook.
XXX.py runs with python3.

At the monment only "HR" part is uploaded. The lennard jones cases is under cleaning, and it will be availible soon. 

"HR_data_xxx.zip" are training/testing data, please extract them first.
"1D_HR_FMT.ipynb" is data generator.  

"train" folder contains "HR_symbolic.py",which is main training code, and it runs by
     python3 HR_symbolic.py arg1 arg2 arg3
with learning rate = arg1 x 10^(-arg2) and running on GPU arg3, set arg3=0 if only one gpu is availible.  

"functional_test" folder contains parameters and functional as in the paper, and tools to test the learned functional.
One doesn't need tensorflow or GPU for this.   
  test_functional.ipynb -> equation of state, minimiztion of test density profiles
  c2_ML.ipynb -> direct correlation function, see Ornsteinâ€“Zernike equation (a good refernece for me is
  chap3 in "Theory of simple liquid", 4th edition)
  functional_expression.ipynb -> latex output of functional and low density expansion

For any question, please send me a mail: shang-chun.lin"at"uni-tuebingen.de

Shang
  
  
  
