This repository is for learnining  functionals from an equation learning network (
see my paper "Analytical classical density functionals from an equation learning network" for details,

arXiv:1910.12752 
or 
The Journal of Chemical Physics: https://aip.scitation.org/doi/10.1063/1.5135919
). 
The main idea is combining "equation learner" (https://github.com/martius-lab) and convolutional neuron network to 
learn functionals, and apply to classical density functional theory problems. 

One need to be somehow familar with python for testing data, and has at least some shallow 
keras/tensorflow knowledge and reasonable hardware (Nvida GPU that can run 10 hours without break) for training.

The crucial libraries are tensorflow(-gpu) 1.12 and sympy 1.3. Nvidia GPU and cuda are strongly recommend for training. 

"tf_file.txt" -> explicity library list for training

One can use conda to creat exactly same enviroment easily (conda create --name name_of_env --file tf_file.txt , 
see https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html for more details).
For testing, even my ultra light laptop can do that (not optimal though). One needs sympy,numpy and some regular libs. 
Tensorflow and GPU are irrelevant for testing.     

XXX.ipynb runs with jupyter-notebook with python3.
XXX.py runs with python3 directly.

HR stands for hard rod and LJ for lennard-jones.

In "HR" folder:
     "HR_data_xxx.zip" are training/testing data, please extract them first.
     "1D_HR_FMT.ipynb" is data generator. Data are generated by HR exact functional.  
     "train" folder contains "HR_symbolic.py", which is the main training code. It runs by
          python3 HR_symbolic.py"
     "functional_test" folder contains parameters and functional as in the paper, and tools to test the learned functional. 
          test_functional.ipynb -> equation of state, minimiztion of test density profiles
          c2_ML.ipynb -> direct correlation function, see Ornsteinâ€“Zernike equation (a good refernece for me is
               chap3 in "Theory of simple liquids", 4th edition)
          functional_expression.ipynb -> latex output of functional and low density expansion
          

In "LJ" folder:
     "xxx.tar.zip" are training/testing/eos/c2 data by simulation/HR exact functional, please extract them first.
               if one want to run LJ data locally, LJ_data.cu is cuda/c code for LJ simulation and should be compiled with 
               Makefile. But it is not the points here so if one is not familiar with c and cuda than probably not worth it.   
               "1D_HR_FMT.ipynb" is data generator by HR exact functional.  
     "LJ_XXX_train" -> contains "LJ_symbolic.ipynb". For the LJ case, I didn't do thorough parameters search, 
               so it is the working parameters but may not always work. Usually I run it 10 times to get 2 working results. 
     "LJ_split_test_xxx" -> the learned functional by "LJ_split_train". "LJ_split_test_paper" is the result I have shown in 
                            the paper. "LJ_split_test_2" is result by same trainning code and give similar performance as 
                            in the paper. As there are some undetermined random seed, result will be different everytime. 
                            XXX.ipynb are named in the same spirit as the HR cases. 
     "LJ_no_split_test" -> the learned functional by "LJ_no_split_train". The result I have shown in the paper. 
     
For any question, please send me a mail: shang-chun.lin"at"uni-tuebingen.de

Shang
  
  
  
